{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c431fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jared/dev/semantic-paper-search/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[ 0.0015,  0.0165, -0.0281,  ..., -0.0309,  0.0297, -0.0327],\n",
      "        [ 0.0151,  0.0041, -0.0157,  ..., -0.0281,  0.0408, -0.0251]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"样例数据-1\", \"样例数据-2\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "model.eval()\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdc6045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0015,  0.0165, -0.0281,  ..., -0.0309,  0.0297, -0.0327])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10014e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0151,  0.0041, -0.0157,  ..., -0.0281,  0.0408, -0.0251])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3da5210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8792)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0] @ sentence_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1331b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[[ 1.,  1.,  1.,  ...,  1.,  1., -1.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"\"\"\n",
    "    \\\\begin{algorithm}[h]\n",
    "        \\\\caption{$(3+\\\\epsilon)$-Approximation for Minimum-Norm Capacitated $k$-Clustering}\n",
    "        \\\\label{algo:MNCkC}\n",
    "        \\\\begin{algorithmic}[1]\n",
    "            \\\\State define set $\\\\bfT$ of non-negative reals with $|\\\\bfT| \\\\leq O\\\\left(\\\\frac{\\\\log n}{\\\\epsilon}\\\\right)$, as described in text \\\\label{step:MNCkC-T}\n",
    "            \\\\label{step:MNCkC-bfT}\n",
    "            \\\\For{every $t \\\\in \\\\bfT$} %\\\\Comment{for each $t$, construct a top-$t$ norm capacitated $k$-clustering instance}\n",
    "                \\\\State with objective $h(v) := \\\\sum_{j \\\\in C}(v_j - t)^+$ for every $v \\\\in \\\\R_{\\\\geq 0}^C$, use Theorem~\\\\ref{thm:pseudo-approx} to obtain a set $\\\\calS^t$ of $O\\\\left(\\\\frac{k \\\\ln n}{\\\\epsilon}\\\\right)$ valid stars \\\\label{step:MNCkC-calS}\n",
    "                \\\\State $S^t \\\\gets \\\\{i \\\\in F: \\\\exists J, (i, J) \\\\in \\\\calS^t\\\\}$ \\\\label{step:MNCkC-S}\n",
    "            \\\\EndFor\n",
    "            %\\\\State $S \\\\leftarrow \\\\cup_{t \\\\in \\\\bfT} S^t, R \\\\leftarrow \\\\cup_{t \\\\in \\\\bfT} R^t$\n",
    "            \\\\State randomly choose a color function $\\\\mathtt{color}: F \\\\to [k]$ \\\\label{step:MNCkC-color}\n",
    "            \\\\State guess the types of each color $c \\\\in [k]$ \\\\Comment{each color is of type-1, 2 or 3} \\\\label{step:MNCkC-type}\n",
    "            \\\\For{every $t \\\\in \\\\bfT$}\n",
    "            $R^t \\\\leftarrow \\\\texttt{MNCkC-choose-R}(t)$ \\\\label{step:MNCkC-R} \\\\Comment{clients in $R^t$ are called \\\\emph{representatives}}\n",
    "            \\\\EndFor\n",
    "            \\\\State $S \\\\leftarrow \\\\union_{t \\\\in \\\\bfT} S^t, R \\\\leftarrow \\\\union_{t \\\\in \\\\bfT} R^t$ \\\\label{step:MNCkC-merge}\n",
    "            \\\\State guess a pivot $p_c$ for each type-1 or 2 color $c \\\\in [k]$, and $i^*_c$ for each type-3 color $c \\\\in [k]$ such that \\\\label{step:MNCkC-pivots}\n",
    "            \\\\Statex \\\\Comment{$p_c$'s for type-1 and 2 colors $c$, $i^*_c$'s for all colors $c \\\\in [k]$ are defined in text}\n",
    "            \\\\begin{itemize}\n",
    "                \\\\item $p_c \\\\in R$ if $c$ is of type-1, $p_c \\\\in S$ if $c$ is of type-2, and\n",
    "                \\\\item $i^*_c \\\\in S$ has color $c$ for a type-3 color $c$.\n",
    "            \\\\end{itemize}\n",
    "            \\\\State guess a $(1+\\\\epsilon)$-approximate overestimation $r_c$ for $d(i^*_c, p_c)$ for every type-1 or 2 color $c$ \\\\label{step:MNCkC-radius}\n",
    "            \\\\State \\\\Return $\\\\texttt{MNCkC-clustering-with-pivots}()$ \\\\Comment{See Algorithm \\\\ref{algo:MNCkC-clustering-with-pivots} for its definition} \\\\label{step:MNCkC-return}\n",
    "        \\\\end{algorithmic}\n",
    "    \\\\end{algorithm}\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    \\\\begin{algorithm}[H]\n",
    "    \\\\caption{In-Place N-Dimensional Sub-Tensor Reversal}\n",
    "    \\\\label{alg:reverse}\n",
    "    \\\\begin{algorithmic}[1]\n",
    "    \\\\State \\\\textbf{function} ReverseND(Tensor $T$, StartIndices $\\\\mathbf{s}$, EndIndices $\\\\mathbf{e}$)\n",
    "    \\\\State \\\\quad $n \\\\gets \\\\text{dimensionality of } T$\n",
    "    \\\\State \\\\quad \\\\text{Initialize current index } $\\\\mathbf{i} \\\\gets \\\\mathbf{s}$\n",
    "    \\\\State \\\\quad \\\\textbf{loop}\n",
    "    \\\\State \\\\quad \\\\quad \\\\text{Calculate mirror index } $\\\\mathbf{j} \\\\gets \\\\mathbf{s} + \\\\mathbf{e} - \\\\mathbf{i}$ \\\\Comment{Component-wise operation}\n",
    "    \\\\State \\\\quad \\\\quad \\\\textbf{if} $\\\\mathbf{i}$ is lexicographically $\\\\ge$ $\\\\mathbf{j}$ \\\\textbf{then}\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\textbf{break} \\\\Comment{All pairs have been swapped}\n",
    "    \\\\State \\\\quad \\\\quad \\\\textbf{end if}\n",
    "    \\\\State \\\\quad \\\\quad swap($T[\\\\mathbf{i}], T[\\\\mathbf{j}]$)\n",
    "    \\\\State \\\\quad \\\\quad \\\\Comment{Increment index $\\\\mathbf{i}$ to the next position}\n",
    "    \\\\State \\\\quad \\\\quad $d \\\\gets n - 1$\n",
    "    \\\\State \\\\quad \\\\quad \\\\textbf{while} $d \\\\ge 0$ \\\\textbf{do}\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\textbf{if} $i_d < e_d$ \\\\textbf{then}\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\quad $i_d \\\\gets i_d + 1$\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\quad \\\\textbf{break}\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\textbf{else}\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\quad $i_d \\\\gets s_d$\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\quad $d \\\\gets d - 1$\n",
    "    \\\\State \\\\quad \\\\quad \\\\quad \\\\textbf{end if}\n",
    "    \\\\State \\\\quad \\\\quad \\\\textbf{end while}\n",
    "    \\\\State \\\\quad \\\\quad \\\\textbf{if} $d < 0$ \\\\textbf{then break} \\\\Comment{All indices exhausted}\n",
    "    \\\\State \\\\quad \\\\textbf{end loop}\n",
    "    \\\\State \\\\textbf{end function}\n",
    "    \\\\end{algorithmic}\n",
    "    \\\\end{algorithm}\n",
    "    \"\"\"\n",
    " ]\n",
    "\n",
    "# Process each sentence and collect embeddings\n",
    "all_embeddings = []\n",
    "for sentence in sentences:\n",
    "    embedding = get_embeddings_for_long_text(sentence, tokenizer, model)\n",
    "    all_embeddings.append(embedding)\n",
    "\n",
    "# Stack and normalize\n",
    "sentence_embeddings = torch.stack(all_embeddings)\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17262cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[522.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0] @ sentence_embeddings[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
