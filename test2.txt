You are given the LaTeX source of a research paper. Extract every algorithm defined in the paper. If there are no defined algorithms, output "NO_ALGORITHMS_FOUND".

For each algorithm, rewrite it exactly in LaTeX using the algorithm and algpseudocode packages. Use a consistent modern style with \begin{algorithm} and \begin{algorithmic}[1]. Use \Require for inputs, \Ensure for outputs, \State for steps, \gets for assignment, and \textsc{} for procedure names.

The LaTeX algorithm must have the same number of steps as the original and use the same exact notation, variable names, and symbols. Do not add, remove, merge, or split steps. Preserve the original logic and ordering. Use LaTeX math symbols only (e.g., \zeta), not Unicode characters.

Use the following as a reference for formatting:
\\begin{algorithm}
\\caption{...}
\\begin{algorithmic}[1]
    \\Require ...
    \\Ensure ...
    \\State ...
\\end{algorithmic}
\\end{algorithm}

After the LaTeX block, list and briefly describe every variable, parameter, and constant appearing in the algorithm.

Then write one concise paragraph describing what the algorithm does and its purpose at a high level. The description paragraph must be self-contained. Do not reference other algorithms, later sections, or the role of the algorithm within the paper (e.g., "forms the foundation for", "extended by", "used in Algorithm X"), unless such a reference is strictly required to define an input or output.

Output only the algorithm LaTeX, the variable list, and the description. Do not add commentary or extra formatting.

LaTeX source:
\section{Existing Solutions and Their Defects} \label{sec:existing}

Existing solutions for PPR or HPP calculations cannot be applied directly to the proposed $\epsilon$ approximate single-source AHPP query. This is because they neglect the informative node attribute. Thus, in this section, we adapt several existing methods to suit the $\epsilon$-approximate single-source AHPP query, which can be classified into three distinct groups: Monte Carlo-based, Power Iteration-based, and Forward Push-based. %质量是有保证的，比如target push所以这里就不提质量的问题了。


\subsection{The Monte Carlo Method} \label{subsec:mc}
The Monte Carlo (MC) method \cite{liao2023efficient} is a well-established approach for deriving sample-based estimates of target values through probabilistic simulations. Inspired by interpreting AHPP as $\alpha$-attribute-augmented hidden random walks (Lemma \ref{lem:compute_A-RWR}), the AHPP vector $\boldsymbol{\pi}_u$ can be effectively approximated using the MC method. Furthermore, MC can effectively provide high probability unbiased and accurate estimates of AHPP. The core principle of the MC method involves simulating many random walks from $u$, utilizing the empirical termination distribution as an approximation for $\boldsymbol{\pi}_u$. This approach offers a natural solution for addressing the $\epsilon$-approximate single-source AHPP query. Assume that $\omega$ random walks are generated independently, requiring an expected time of $O(\omega/\alpha)$. According to \cite{liao2023efficient}, setting $\omega = O\left( \frac{2 \left( 1 + \epsilon / 3 \right) \cdot \ln \left( |U| / p_f \right)}{\epsilon^2} \right)$ ensures that for each $u_i \in U$, we have $|\hat{\pi}(u,u_i) - \pi(u,u_i)| < \epsilon$ holds with at least $1 - p_f$ probability.

\stitle{Limitations of MC.} While the MC can yield high-probability, unbiased, and accurate estimates of AHPP, it is relatively inefficient due to the need for sampling a large number of random walks. Furthermore, to enable random walk sampling on weighted graphs, MC requires constructing alias structures \cite{liao2023efficient} for each node's neighborhood during the preprocessing phase, resulting in significant computational overhead.


\subsection{The Power Iteration Method} \label{subsec:pi}
The Power Iteration (PI) method \cite{wang2017fora} is a fundamental iterative method for computing the entire AHPP vector $\boldsymbol{\pi}_u$, as originally introduced in Google's seminal work. Specifically, PI estimates AHPP values by iteratively solving the following linear system (a variant of Eq. \eqref{eq:random_walk_style}): $\boldsymbol{\pi}_u = (1-\alpha)\cdot\boldsymbol{\pi}_u\cdot\mathbf{P} + \alpha\cdot\mathbf{e}_u$,
where the one-hot vector $\mathbf{e}_u \in \mathbb{R}^{1 \times |U|}$ takes a value of 1 at the $u$-th position and 0 elsewhere, $\boldsymbol{\pi}_u(u_i) = \pi(u,u_i)$ for all $u_i \in U$, and $\mathbf{P} = (1 - \beta) \cdot \mathbf{P_S} + \beta \cdot \mathbf{P_A}$. Algorithm \ref{alg:PI} provides the pseudo-code for the PI method to approximate $\boldsymbol{\pi}_u$ given the input graph $G$ and the source node $u$. It is noteworthy that, in the worst-case scenario, the number of non-zero entries in the transition matrix $\mathbf{P}$ can scale as $O(|U|^2)$. As a result, the computational cost of matrix-vector multiplications in each iteration is $O(|U|^2)$. According to \cite{wang2017fora}, PI guarantees an absolute error bound of $\epsilon$ for each $\pi(u, u_i)$ after $T = \log_{\frac{1}{1 - \alpha}} \frac{1}{\epsilon}$ iterations of matrix multiplications. Thus, PI resolves the single-source AHPP query within an absolute error bound of $\epsilon$ in $O(|U|^2 \cdot T) = O(|U|^2 \cdot \log(1/\epsilon))$ time.

\stitle{Limitations of PI.} Although PI is straightforward to implement, it becomes inefficient on large graphs due to its \( O(|U|^2) \) time and space complexities. Additionally, when the error threshold (i.e., \( \epsilon \)) is small, PI requires a large number of iterations of matrix-vector multiplication, which can be extremely computationally expensive.





\begin{algorithm}[t!]%\footnotesize
    \caption{Power Iteration (PI)}
    \label{alg:PI}
    \KwIn{Attributed bipartite graph $G$, source node $u$, restart probability $\alpha$, attribute jumping probability $\beta$, the number of iterations $T$.}
    \KwOut{$\boldsymbol{\pi}_u$.}
    $\boldsymbol{\pi}_u \gets \mathbf{e}_u$\;
    \For{$i \gets 1$ to $T$} {
        $\boldsymbol{\pi}_u \gets (1 - \alpha) \cdot ((1 - \beta) \cdot \boldsymbol{\pi}_u \cdot \mathbf{P_S} + \beta \cdot \boldsymbol{\pi}_u \cdot \mathbf{P_A}) + \alpha \cdot \mathbf{e}_u$\;
    }
    \Return{$\boldsymbol{\pi}_u$}\;
\end{algorithm}







\begin{algorithm}[t!]%\footnotesize
    \caption{Forward Push (FP)}
    \label{alg:FP}
    \KwIn{Attributed bipartite graph $G$, source node $u$, restart probability $\alpha$, attribute jumping probability $\beta$, residue threshold $r_{max}$.}
    \KwOut{$\{ \hat{\pi}(u, u_i) \mid u_i \in U \}$.}
    $r(u, u) \gets 1$; $r(u, u_i) \gets 0 \ \forall{u_i \in U\setminus \{u\}}$\;
    $\hat{\pi}(u, u_i) \gets 0 \ \forall{u_i \in U}$\;
    \While {$\exists u_i \in U$ s.t. $r(u, u_i)> r_{max} \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)$} {
        $\hat{\pi}(u, u_i) \gets \hat{\pi}(u, u_i) + \alpha \cdot r(u, u_i)$\;
        $\rho \gets (1 - \alpha) \cdot r(u, u_i)$\;
        $r(u, u_i) \gets 0$\;
        \For {each $v \in N(u_i)$} {
            \For {each $u_j \in N(v)$} {
                $r(u, u_j) \gets r(u, u_j) + (1 - \beta) \cdot \frac{w(u_i, v) \cdot w(v, u_j)}{d(u_i) \cdot d(v)} \cdot \rho$\;
            }
        }
        \For {each $a \in \mathcal{A}(u_i)$} {
            \For {each $u_j \in \mathcal{A}^{-1}(a)$} {
                $r(u, u_j) \gets r(u, u_j) + \frac{\beta \cdot w(u_i, a) \cdot w(u_j, a)\cdot \rho}{\sum_{a_x \in \mathcal{A}(u_i)}{w(u_i, a_x)} \cdot \sum_{u_x \in \mathcal{A}^{-1}(a)}{w(u_x, a)}}$\;
            }
        }
    }
    \Return{$\{ \hat{\pi}(u, u_i) \mid u_i \in U \}$}\;
\end{algorithm}












\subsection{The Forward Push Method} \label{subsec:fp}
The Forward Push (FP) method  \cite{wang2017fora} is a local-push method capable of answering single-source AHPP queries without searching the whole graph. The fundamental idea behind FP is to simulate numerous random walks deterministically by distributing the probability mass from a node to its neighbors. Specifically, given a source node $u$ and a parameter $r_{max} \in [0, 1]$, FP maintains the following information throughout the execution process:
\begin{itemize}
    \item a reserve $\hat{\pi}(u, u_i)$ for all $u_i \in U$: the probability mass that remains at node $u_i$, it is an underestimate of $\pi(u, u_i)$;
    \item a residue $r(u, u_i)$ for all $u_i \in U$: \textcolor{blue}{The probability mass on node $u_i$ will be redistributed to other nodes. In a random walk, $r(u, u_i)$ represents the mass from a walk starting at $u$ that is still alive at $u_i$, where a walk is alive if it has not yet terminated.}
\end{itemize}

\textcolor{blue}{Algorithm~\ref{alg:FP} presents the FP method. It initializes the residue $r(u, u) = 1$ for the source node $u$ and $0$ for all other nodes, and sets the approximate AHPP values $\hat{\pi}(u, u_i) = 0$ for all $u_i \in U$ (Lines 1--2). Iteratively, residues of selected nodes are distributed to their two-hop neighbors and nodes sharing attributes. For a node $u_i$ with large residue ($r(u, u_i) > r_{max} \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)$), the AHPP value is updated as $\hat{\pi}(u, u_i) \mathrel{+}= \alpha \cdot r(u, u_i)$. Each two-hop neighbor $u_j \in \bigcup_{v \in N(u_i)} N(v)$ receives $(1 - \alpha)(1 - \beta) \frac{w(u_i, v) w(v, u_j)}{d(u_i) d(v)} r(u, u_i)$ (Lines 7--9), and each node $u_k$ sharing attributes with $u_i$ receives $(1 - \alpha)\beta \frac{w(u_i, a) w(u_k, a)}{\sum_{a_x \in \mathcal{A}(u_i)} w(u_i, a_x) \sum_{u_x \in \mathcal{A}^{-1}(a)} w(u_x, a)} r(u, u_i)$ (Lines 10--12). After processing, $r(u, u_i)$ is set to zero (Line 6). These steps repeat until all nodes satisfy $r(u, u_i) \le r_{max} \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)$.}

%\stitle{Time Complexity.} Lemma \ref{lem:fwd_time_complexity} presents the time cost of FP.
\iffalse
\begin{lemma}\label{lem:fwd_time_complexity}
The time complexity of Algorithm \ref{alg:FP} is $O(\sum_{u_i \in U}\\{(\frac{\pi(u, u_i) \cdot (|E| + |\mathcal{A}|)}{\alpha \cdot \epsilon \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)} \cdot (\sum_{v \in N(u_i)}{|N(v)|} + \sum_{a \in \mathcal{A}(u_i)}{|\mathcal{A}^{-1}(a)|}))})$
\end{lemma}
\fi


\begin{lemma}\label{lem:fwd_time_complexity}
Given the query node $u$, the time complexity of Algorithm \ref{alg:FP} is $O(\sum_{u_i \in U}\frac{\pi(u, u_i)\cdot (\sum_{v \in N(u_i)}{|N(v)|} + \sum_{a \in \mathcal{A}(u_i)}{|\mathcal{A}^{-1}(a)|})}{\alpha \cdot r_{max} \cdot(|N(u_i)| + |\mathcal{A}(u_i)|)}).$
\end{lemma}

%The explanation establishes the basis for Algorithm \ref{alg:FP}'s time complexity by analyzing the operations performed in each iteration and relating them to the algorithm's absolute error guarantee.


\begin{proof}
In each iteration for a selected node $u_i$ (Line 3), a portion $\alpha$ of the residue $r(u, u_i) > r_{max} \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)$ is converted to its approximate AHPP $\pi(u, u_i)$, while the remaining residue is distributed among two-hop neighbors $\bigcup_{v \in N(u_i)}{N(v)}$ as well as nodes sharing attributes with $u_i$. Since $\hat{\pi}(u, u_i) \le \pi(u, u_i)$, the number of iterations for $u_i$ to convert its residue entirely is bounded by $\frac{\pi(u, u_i)}{\alpha \cdot r_{max} \cdot (|N(u_i)| + |\mathcal{A}(u_i)|)}$. Each iteration for $u_i$ has a cost proportional to $(\sum_{v \in N(u_i)}{|N(v)|} + \sum_{a \in \mathcal{A}(u_i)}{|\mathcal{A}^{-1}(a)|})$. Thus, the cost for $u_i$  is $\frac{\pi(u, u_i)\cdot (\sum_{v \in N(u_i)}{|N(v)|} + \sum_{a \in \mathcal{A}(u_i)}{|\mathcal{A}^{-1}(a)|})}{\alpha \cdot r_{max} \cdot(|N(u_i)| + |\mathcal{A}(u_i)|)}$.
The total time cost for all nodes $u_i \in U$ is obtained by summing the above expression overall $u_i$.
\end{proof}

